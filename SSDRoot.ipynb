{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmzn304mMAN8JlKAAtH0R1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanuMahee12/ayush-vision/blob/ssd/SSDRoot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch6Ma7g9LOi-",
        "outputId": "a0f6081c-8add-4ac8-c387-53c107c68eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.45-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.53.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Downloading roboflow-1.1.45-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-dotenv, idna, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 requests-toolbelt-1.0.0 roboflow-1.1.45\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in ayushvision_root-2 to coco:: 100%|██████████| 23946/23946 [00:01<00:00, 17016.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to ayushvision_root-2 in coco:: 100%|██████████| 488/488 [00:00<00:00, 5402.35it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"KvYzBayallmQzuRDfNc3\")\n",
        "project = rf.workspace(\"ayushvision\").project(\"ayushvision_root\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"coco\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBBbz8mILTrJ",
        "outputId": "0a382839-3ba9-4a18-c296-c3b057e19525"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install roboflow torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cevzH8lMKFD",
        "outputId": "8bf8bbce-b9b2-43ae-d3e6-c8b10e8d5fea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.45)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.53.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = f\"/content/ayushvision_root-2/train\"\n",
        "valid_dir = f\"/content/ayushvision_root-2/valid\""
      ],
      "metadata": {
        "id": "ZAz2pCB6MMET"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "import torchvision.transforms as T\n",
        "\n",
        "transform =T.Compose([\n",
        "    T.Resize((300, 300)),  # Resize to expected input size\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load COCO format dataset\n",
        "train_dataset = CocoDetection(root=train_dir, annFile=f\"{train_dir}/_annotations.coco.json\", transform=transform)\n",
        "valid_dataset = CocoDetection(root=valid_dir, annFile=f\"{valid_dir}/_annotations.coco.json\", transform=transform)\n",
        "\n",
        "# DataLoader to load data in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GufeQooBMaqg",
        "outputId": "e3391124-cde0-4450-a9c8-7aad02cd7985"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained SSD model and modify it for your custom dataset\n",
        "weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
        "model = ssdlite320_mobilenet_v3_large(weights=weights)\n",
        "\n",
        "# Define number of classes (including background)\n",
        "num_classes = len(train_dataset.coco.getCatIds()) + 1  # Number of classes + background\n",
        "\n",
        "# Modify the classification head to match the number of classes\n",
        "# Access the classifier directly and replace it with a new one\n",
        "# Get the input channels from the first layer of the classification head\n",
        "# Check the structure of model.head.classification_head for Roboflow models\n",
        "# It might be different from the torchvision implementation\n",
        "print(model.head.classification_head)\n",
        "\n",
        "# Instead of accessing conv[0], try accessing the first layer directly\n",
        "# Check the model's structure to confirm the correct layer name\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ao4P7PSMh20",
        "outputId": "6dba21a5-05e9-4c47-fcaa-c51983c96a4a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSDLiteClassificationHead(\n",
            "  (module_list): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
            "        (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2d(672, 546, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "        (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2d(480, 546, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2d(512, 546, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (3-4): 2 x Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2d(256, 546, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (2): ReLU6(inplace=True)\n",
            "      )\n",
            "      (1): Conv2d(128, 546, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision import transforms as T\n",
        "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
        "\n",
        "# Define transformations\n",
        "transform =T.Compose([\n",
        "    T.Resize((300, 300)),  # Resize to expected input size\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)  # Unzip the batch into images and targets\n",
        "    return list(images), list(targets)\n",
        "\n",
        "# Load COCO dataset in Roboflow directory structure\n",
        "train_dataset = CocoDetection(root=train_dir, annFile=f\"{train_dir}/_annotations.coco.json\", transform=transform)\n",
        "valid_dataset = CocoDetection(root=valid_dir, annFile=f\"{valid_dir}/_annotations.coco.json\", transform=transform)\n",
        "\n",
        "# Use DataLoader with the custom collate function\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "# Define number of classes (including background)\n",
        "num_classes = len(train_dataset.coco.getCatIds()) + 1  # Number of classes + background\n",
        "# Load the pre-trained SSD model and modify it for your custom dataset\n",
        "weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
        "model = ssdlite320_mobilenet_v3_large(weights=weights)\n",
        "\n",
        "\n",
        "\n",
        "# Get the input channels from the first layer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql4nIyhxPO8J",
        "outputId": "f85952ce-cc04-4e5f-8379-64b4a78826b1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(model.head.classification_head.module_list)):\n",
        "    conv_layer = model.head.classification_head.module_list[i][1]  # Get the Conv2d layer\n",
        "    model.head.classification_head.module_list[i][1] = torch.nn.Conv2d(\n",
        "        in_channels=conv_layer.in_channels,\n",
        "        out_channels=num_classes * 4,  # 4 anchors per bounding box\n",
        "        kernel_size=conv_layer.kernel_size,\n",
        "        stride=conv_layer.stride,\n",
        "        padding=conv_layer.padding\n",
        "    )"
      ],
      "metadata": {
        "id": "rbOKnMOKPhuA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "PIFXGp2wPSKv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "uva-feZwSNvx"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "\n",
        "        new_targets = []\n",
        "        for target_list in targets:  # targets is a list of lists\n",
        "            for t in target_list:  # target_list is a list of dictionaries\n",
        "                new_target = {}\n",
        "\n",
        "                # Extract boxes and labels\n",
        "                boxes = torch.tensor(t['bbox']).reshape(-1, 4)  # Assuming 'bbox' is a list of [x_min, y_min, x_max, y_max]\n",
        "\n",
        "                # Validate boxes: Keep only boxes with positive width and height\n",
        "                valid_boxes = []\n",
        "                for box in boxes:\n",
        "                    x_min, y_min, x_max, y_max = box\n",
        "                    if x_max > x_min and y_max > y_min:  # Valid box condition\n",
        "                        valid_boxes.append(box)\n",
        "\n",
        "                if valid_boxes:  # If there are valid boxes\n",
        "                    new_target['boxes'] = torch.stack(valid_boxes).to(device)  # Move valid boxes to device\n",
        "                else:\n",
        "                    continue  # Skip if no valid boxes\n",
        "\n",
        "                # Extract labels (make sure it corresponds to valid boxes)\n",
        "                labels = torch.tensor([t['category_id']] * len(valid_boxes))  # Assuming category_id applies to all valid boxes\n",
        "                new_target['labels'] = labels.to(device)  # Move labels to device\n",
        "\n",
        "                new_targets.append(new_target)\n",
        "\n",
        "        if not new_targets:  # If no valid targets, skip this iteration\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, new_targets)\n",
        "        total_loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "r3bjIFeDQXw5",
        "outputId": "0595a511-e453-4cfe-9aba-ffe0e7589ce7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[8, -1, 91, 20, 20]' is invalid for input of size 38400",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-868c31a63a64>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/ssd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# compute the ssd heads outputs using the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mhead_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# create the set of anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/ssdlite.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m         return {\n\u001b[1;32m     91\u001b[0m             \u001b[0;34m\"bbox_regression\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;34m\"cls_logits\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         }\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/ssd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# Permute output from (N, A * K, H, W) to (N, HWA, K).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_columns\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Size=(N, HWA, K)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, -1, 91, 20, 20]' is invalid for input of size 38400"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision import transforms as T\n",
        "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
        "\n",
        "# Define transformations\n",
        "transform =T.Compose([\n",
        "    T.Resize((300, 300)),  # Resize to expected input size\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)  # Unzip the batch into images and targets\n",
        "    return list(images), list(targets)\n",
        "\n",
        "# Load COCO dataset in Roboflow directory structure\n",
        "train_dataset = CocoDetection(root=train_dir, annFile=f\"{train_dir}/_annotations.coco.json\", transform=transform)\n",
        "valid_dataset = CocoDetection(root=valid_dir, annFile=f\"{valid_dir}/_annotations.coco.json\", transform=transform)\n",
        "\n",
        "# Use DataLoader with the custom collate function\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "# Define number of classes (including background)\n",
        "num_classes = len(train_dataset.coco.getCatIds()) + 1  # Number of classes + background\n",
        "# Load the pre-trained SSD model and modify it for your custom dataset\n",
        "weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
        "model = ssdlite320_mobilenet_v3_large(weights=weights)\n",
        "for i in range(len(model.head.classification_head.module_list)):\n",
        "    conv_layer = model.head.classification_head.module_list[i][1]  # Get the Conv2d layer\n",
        "    model.head.classification_head.module_list[i][1] = torch.nn.Conv2d(\n",
        "        in_channels=conv_layer.in_channels,\n",
        "        out_channels=num_classes * 4,  # 4 anchors per bounding box\n",
        "        kernel_size=conv_layer.kernel_size,\n",
        "        stride=conv_layer.stride,\n",
        "        padding=conv_layer.padding\n",
        "    )\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "\n",
        "        new_targets = []\n",
        "        for target_list in targets:  # targets is a list of lists\n",
        "            for t in target_list:  # target_list is a list of dictionaries\n",
        "                new_target = {}\n",
        "\n",
        "                # Extract boxes and labels\n",
        "                boxes = torch.tensor(t['bbox']).reshape(-1, 4)  # Assuming 'bbox' is a list of [x_min, y_min, x_max, y_max]\n",
        "\n",
        "                # Validate boxes: Keep only boxes with positive width and height\n",
        "                valid_boxes = []\n",
        "                for box in boxes:\n",
        "                    x_min, y_min, x_max, y_max = box\n",
        "                    if x_max > x_min and y_max > y_min:  # Valid box condition\n",
        "                        valid_boxes.append(box)\n",
        "\n",
        "                if valid_boxes:  # If there are valid boxes\n",
        "                    new_target['boxes'] = torch.stack(valid_boxes).to(device)  # Move valid boxes to device\n",
        "                else:\n",
        "                    continue  # Skip if no valid boxes\n",
        "\n",
        "                # Extract labels (make sure it corresponds to valid boxes)\n",
        "                labels = torch.tensor([t['category_id']] * len(valid_boxes))  # Assuming category_id applies to all valid boxes\n",
        "                new_target['labels'] = labels.to(device)  # Move labels to device\n",
        "\n",
        "                new_targets.append(new_target)\n",
        "\n",
        "        if not new_targets:  # If no valid targets, skip this iteration\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, new_targets)\n",
        "        total_loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEOfJQbcUmLI",
        "outputId": "0addb23d-b111-408b-c044-2d026c6ed1f2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision import transforms as T\n",
        "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
        "import numpy as np\n",
        "\n",
        "# Define transformations\n",
        "transform = T.Compose([\n",
        "    T.Resize((300, 300)),  # Resize to expected input size\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)  # Unzip the batch into images and targets\n",
        "    return list(images), list(targets)\n",
        "\n",
        "# Load COCO dataset in Roboflow directory structure\n",
        "train_dir = \"/content/ayushvision_root-2/train\"\n",
        "valid_dir = \"/content/ayushvision_root-2/valid\"\n",
        "train_dataset = CocoDetection(root=train_dir, annFile=f\"{train_dir}/_annotations.coco.json\", transform=transform)\n",
        "valid_dataset = CocoDetection(root=valid_dir, annFile=f\"{valid_dir}/_annotations.coco.json\", transform=transform)\n",
        "\n",
        "# Use DataLoader with the custom collate function\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "\n",
        "# Define number of classes (1 for the plant + 1 for background)\n",
        "num_classes = 2  # 1 for plant + 1 for background\n",
        "\n",
        "# Load the pre-trained SSD model and modify it for your custom dataset\n",
        "weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
        "model = ssdlite320_mobilenet_v3_large(weights=weights)\n",
        "\n",
        "# Modify the model's classification head for the new number of classes\n",
        "model.head.classification_head.num_classes = num_classes  # Set to the number of classes\n",
        "\n",
        "# Move the model to GPU or CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to calculate IoU\n",
        "def calculate_iou(box, boxes):\n",
        "    box = box.reshape(1, -1)  # Reshape to (1, 4) for calculations\n",
        "    x1 = np.maximum(box[:, 0], boxes[:, 0])\n",
        "    y1 = np.maximum(box[:, 1], boxes[:, 1])\n",
        "    x2 = np.minimum(box[:, 2], boxes[:, 2])\n",
        "    y2 = np.minimum(box[:, 3], boxes[:, 3])\n",
        "\n",
        "    inter_area = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n",
        "    box_area = (box[0][2] - box[0][0]) * (box[0][3] - box[0][1])\n",
        "    boxes_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "\n",
        "    union_area = box_area + boxes_area - inter_area\n",
        "    iou = inter_area / union_area\n",
        "    iou[np.isnan(iou)] = 0  # Handle division by zero\n",
        "\n",
        "    return iou\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(predictions, targets, threshold=0.5):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for target, prediction in zip(targets, predictions):\n",
        "        if prediction is not None and len(prediction['boxes']) > 0:\n",
        "            pred_boxes = prediction['boxes'].cpu().numpy()\n",
        "            pred_labels = prediction['labels'].cpu().numpy()\n",
        "\n",
        "            # Gather true boxes for the current target\n",
        "            true_boxes = torch.tensor([t['bbox'] for t in target]).reshape(-1, 4).cpu().numpy()\n",
        "\n",
        "            for pred_box in pred_boxes:\n",
        "                # Calculate IoU with true boxes\n",
        "                iou = calculate_iou(pred_box, true_boxes)\n",
        "                # Check if there is at least one IoU that exceeds the threshold\n",
        "                if np.any(iou >= threshold):\n",
        "                    correct_predictions += 1\n",
        "            total_predictions += len(pred_boxes)\n",
        "\n",
        "    return correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "\n",
        "        new_targets = []\n",
        "        for target_list in targets:  # targets is a list of lists\n",
        "            for t in target_list:  # target_list is a list of dictionaries\n",
        "                new_target = {}\n",
        "\n",
        "                # Extract boxes and labels\n",
        "                boxes = torch.tensor(t['bbox']).reshape(-1, 4)  # Assuming 'bbox' is a list of [x_min, y_min, width, height]\n",
        "\n",
        "                # Validate boxes: Keep only boxes with positive width and height\n",
        "                valid_boxes = []\n",
        "                for box in boxes:\n",
        "                    x_min, y_min, width, height = box\n",
        "                    x_max = x_min + width\n",
        "                    y_max = y_min + height\n",
        "                    if x_max > x_min and y_max > y_min:  # Valid box condition\n",
        "                        valid_boxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "                if valid_boxes:  # If there are valid boxes\n",
        "                    new_target['boxes'] = torch.tensor(valid_boxes).to(device)  # Move valid boxes to device\n",
        "                    new_target['labels'] = torch.tensor([1] * len(valid_boxes)).to(device)  # Class ID of the detected plant\n",
        "                    new_targets.append(new_target)\n",
        "\n",
        "        if not new_targets:  # If no valid targets, skip this iteration\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, new_targets)\n",
        "        total_loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "\n",
        "    # Evaluate accuracy at the end of each epoch\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets_list = []  # Collect targets for comparison\n",
        "    for images, targets in valid_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets_list.extend(targets)  # Collect targets for accuracy calculation\n",
        "        with torch.no_grad():\n",
        "            pred = model(images)\n",
        "        predictions.extend(pred)\n",
        "\n",
        "    accuracy = calculate_accuracy(predictions, targets_list)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfGpOlgncEGB",
        "outputId": "6e746858-bb99-4b48-a9c2-e0a73bb6ae60"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Epoch [1/50], Loss: 29.8369, Accuracy: 0.0073\n",
            "Epoch [2/50], Loss: 15.4224, Accuracy: 0.0023\n",
            "Epoch [3/50], Loss: 13.7822, Accuracy: 0.0037\n",
            "Epoch [4/50], Loss: 13.4805, Accuracy: 0.0053\n",
            "Epoch [5/50], Loss: 12.0782, Accuracy: 0.0075\n",
            "Epoch [6/50], Loss: 12.5709, Accuracy: 0.0038\n",
            "Epoch [7/50], Loss: 12.7685, Accuracy: 0.0025\n",
            "Epoch [8/50], Loss: 11.7870, Accuracy: 0.0021\n",
            "Epoch [9/50], Loss: 12.9900, Accuracy: 0.0021\n",
            "Epoch [10/50], Loss: 10.8886, Accuracy: 0.0018\n",
            "Epoch [11/50], Loss: 12.1743, Accuracy: 0.0017\n",
            "Epoch [12/50], Loss: 11.8598, Accuracy: 0.0039\n",
            "Epoch [13/50], Loss: 11.8844, Accuracy: 0.0046\n",
            "Epoch [14/50], Loss: 11.3878, Accuracy: 0.0049\n",
            "Epoch [15/50], Loss: 11.9371, Accuracy: 0.0045\n",
            "Epoch [16/50], Loss: 11.8020, Accuracy: 0.0035\n",
            "Epoch [17/50], Loss: 11.5981, Accuracy: 0.0041\n",
            "Epoch [18/50], Loss: 10.8004, Accuracy: 0.0039\n",
            "Epoch [19/50], Loss: 11.6428, Accuracy: 0.0035\n",
            "Epoch [20/50], Loss: 10.0655, Accuracy: 0.0027\n",
            "Epoch [21/50], Loss: 12.6013, Accuracy: 0.0018\n",
            "Epoch [22/50], Loss: 10.6880, Accuracy: 0.0020\n",
            "Epoch [23/50], Loss: 11.3359, Accuracy: 0.0022\n",
            "Epoch [24/50], Loss: 11.0346, Accuracy: 0.0027\n",
            "Epoch [25/50], Loss: 12.1583, Accuracy: 0.0018\n",
            "Epoch [26/50], Loss: 12.2660, Accuracy: 0.0015\n",
            "Epoch [27/50], Loss: 11.8216, Accuracy: 0.0019\n",
            "Epoch [28/50], Loss: 11.6128, Accuracy: 0.0028\n",
            "Epoch [29/50], Loss: 11.6524, Accuracy: 0.0059\n",
            "Epoch [30/50], Loss: 11.1449, Accuracy: 0.0049\n",
            "Epoch [31/50], Loss: 12.0408, Accuracy: 0.0050\n",
            "Epoch [32/50], Loss: 10.9657, Accuracy: 0.0046\n",
            "Epoch [33/50], Loss: 10.7825, Accuracy: 0.0043\n",
            "Epoch [34/50], Loss: 11.3810, Accuracy: 0.0028\n",
            "Epoch [35/50], Loss: 11.2217, Accuracy: 0.0034\n",
            "Epoch [36/50], Loss: 11.9786, Accuracy: 0.0036\n",
            "Epoch [37/50], Loss: 12.5038, Accuracy: 0.0037\n",
            "Epoch [38/50], Loss: 10.9574, Accuracy: 0.0027\n",
            "Epoch [39/50], Loss: 11.2061, Accuracy: 0.0027\n",
            "Epoch [40/50], Loss: 11.7126, Accuracy: 0.0046\n",
            "Epoch [41/50], Loss: 9.7032, Accuracy: 0.0038\n",
            "Epoch [42/50], Loss: 11.6816, Accuracy: 0.0038\n",
            "Epoch [43/50], Loss: 11.7066, Accuracy: 0.0049\n",
            "Epoch [44/50], Loss: 12.1981, Accuracy: 0.0044\n",
            "Epoch [45/50], Loss: 11.5617, Accuracy: 0.0039\n",
            "Epoch [46/50], Loss: 12.2720, Accuracy: 0.0043\n",
            "Epoch [47/50], Loss: 11.0938, Accuracy: 0.0037\n",
            "Epoch [48/50], Loss: 10.3685, Accuracy: 0.0046\n",
            "Epoch [49/50], Loss: 11.9885, Accuracy: 0.0045\n",
            "Epoch [50/50], Loss: 12.1504, Accuracy: 0.0049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation phase\n",
        "model.eval()\n",
        "predictions = []\n",
        "targets_list = []  # Collect targets for comparison\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, targets in valid_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets_list.extend(targets)  # Collect targets for accuracy calculation\n",
        "        pred = model(images)\n",
        "        predictions.extend(pred)\n",
        "\n",
        "# Ensure predictions and targets_list have the same length\n",
        "print(f\"Number of predictions: {len(predictions)}, Number of targets: {len(targets_list)}\")\n",
        "\n",
        "# Filter out empty predictions and collect corresponding targets\n",
        "filtered_predictions = []\n",
        "filtered_targets = []  # Collect corresponding targets for valid predictions\n",
        "for prediction, target in zip(predictions, targets_list):\n",
        "    if prediction is not None and len(prediction['boxes']) > 0:\n",
        "        filtered_predictions.append(prediction)\n",
        "        filtered_targets.append(target)\n",
        "\n",
        "# Ensure filtered predictions and targets have the same length\n",
        "print(f\"Filtered number of predictions: {len(filtered_predictions)}, Filtered number of targets: {len(filtered_targets)}\")\n",
        "\n",
        "# Now calculate metrics\n",
        "accuracy, precision, recall, f1 = calculate_metrics(filtered_predictions, filtered_targets)\n",
        "print(f\"Evaluation Results: \"\n",
        "      f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "B6E6_Edid3Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uteDBB19Vx6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}