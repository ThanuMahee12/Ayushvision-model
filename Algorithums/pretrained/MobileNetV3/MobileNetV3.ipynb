{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanuMahee12/ayush-vision/blob/mobile-net/Algorithums/pretrained/MobileNetV3/MobileNetV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bowAAElvCDW"
      },
      "source": [
        "# MobileNet v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eTv_O1kvJwv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB3vvwLyvkwX"
      },
      "source": [
        "### Collab Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bkWRg7-mvI4B"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNIqJkA0wK6i"
      },
      "source": [
        "### Default Nessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GFVIoWAswFdb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsz1l9cewVxo"
      },
      "source": [
        "### Tensorflow Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1870sUacwaM2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import MobileNetV3Large\n",
        "from tensorflow.keras.layers import Dense,Conv2D, MaxPooling2D, Flatten,GlobalAveragePooling2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehXo5c_80EOM"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9ZRN4Tx0LCL"
      },
      "source": [
        "### Path variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EMEcZoJt3J6E"
      },
      "outputs": [],
      "source": [
        "dataset_dir='/content/drive/MyDrive/AYUSHVISION/working' #datasetbase dirctory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MEP4zYoc3W0y"
      },
      "outputs": [],
      "source": [
        "dataset_train_dir=os.path.join(dataset_dir,'train')\n",
        "dataset_test_dir=os.path.join(dataset_dir,'val')\n",
        "dataset_val_dir=os.path.join(dataset_dir,'val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifz4Gosy7Jf0"
      },
      "source": [
        "### Genrator Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WxzxLjTJ7N2R"
      },
      "outputs": [],
      "source": [
        "rescale=1/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noQZt9AT8s39"
      },
      "source": [
        "### Configration Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0M5APq8U8wdj"
      },
      "outputs": [],
      "source": [
        "target_size=(224,224)\n",
        "class_mode='categorical'\n",
        "input_shape=(224,224,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxx_ewkNvGEP"
      },
      "source": [
        "### Model Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIv_Hh05vJ-M"
      },
      "source": [
        "#### Number If Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1rVO9kLQvNF7"
      },
      "outputs": [],
      "source": [
        "no_of_classes=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAY9WhFNxI5l"
      },
      "source": [
        "## Envirumentatl Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3TJCwCUxNiP"
      },
      "source": [
        "### Collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VKvkxb2xxP6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9545df0-ee25-4dba-bde1-cc5f2b6e04f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive') # drive activate for Use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rPbqEgK1Wr8"
      },
      "source": [
        "## Validations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "404OTbpS4ysB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "06179c0d-d791-490d-e708-6b875a3b305b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/AYUSHVISION/working/train is Correct'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "f'{dataset_train_dir} is Correct' if os.path.exists(dataset_train_dir) and os.path.isdir(dataset_train_dir) else f' {dataset_train_dir} is incorrect'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qg53pP8q5TxB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1ab2b14d-d570-4556-f978-3df3f70cc500"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/AYUSHVISION/working/val is Correct'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "f'{dataset_test_dir} is Correct' if os.path.exists(dataset_test_dir) and os.path.isdir(dataset_train_dir) else f' {dataset_test_dir} is incorrect'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2YeeG1CK5VaE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ff43bd59-bd84-4279-ec8d-7a99bf81c317"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/AYUSHVISION/working/val is Correct'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "f'{dataset_val_dir} is Correct' if os.path.exists(dataset_val_dir) and os.path.isdir(dataset_val_dir) else f' {dataset_val_dir} is incorrect'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fTiill8zzef"
      },
      "source": [
        "## DataSet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2BunOax5fWk"
      },
      "source": [
        "### Image Genarator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Wbnr0KTw6sOn"
      },
      "outputs": [],
      "source": [
        "train_data_genrator=image.ImageDataGenerator(rescale=rescale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hsLpWRvK7U1X"
      },
      "outputs": [],
      "source": [
        "test_data_genrator=image.ImageDataGenerator(rescale=rescale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XoTIixHF7XyY"
      },
      "outputs": [],
      "source": [
        "val_data_genrator=image.ImageDataGenerator(rescale=rescale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxYUE9dG8__v"
      },
      "source": [
        "### Prepared Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bnj4OQce9D9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0019668c-2fe6-4adf-cdfb-19a9057d7473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 140 images belonging to 1 classes.\n",
            "Found 20 images belonging to 1 classes.\n",
            "Found 20 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "train_ds_gen=train_data_genrator.flow_from_directory(\n",
        "    dataset_train_dir,\n",
        "    target_size=target_size,\n",
        "    class_mode=class_mode\n",
        ")\n",
        "val_ds_gen=val_data_genrator.flow_from_directory(\n",
        "    dataset_val_dir,\n",
        "    target_size=target_size,\n",
        "    class_mode=class_mode\n",
        ")\n",
        "\n",
        "test_ds_gen=test_data_genrator.flow_from_directory(\n",
        "    dataset_test_dir,\n",
        "    target_size=target_size,\n",
        "    class_mode=class_mode\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaWo1hLltAaS"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exq_qfzttIhc"
      },
      "source": [
        "### Base Model MobileNetV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Y7YWxKditGcq"
      },
      "outputs": [],
      "source": [
        "ayush_mobilenetV3_base=MobileNetV3Large(input_shape=input_shape,  # We don't want the dense layers on top\n",
        "                       weights=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k20JYY3etkBS"
      },
      "source": [
        "### Layers setup for Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TpS2tukutrxR"
      },
      "outputs": [],
      "source": [
        "ayush_mobilenetV3_base.trainable=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lO56BpGt2wa"
      },
      "source": [
        "### Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GIdrUMvvt5B-"
      },
      "outputs": [],
      "source": [
        "ayushVision_MobileNetV3_Model=Sequential([ayush_mobilenetV3_base])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q4oSgBeuQxF"
      },
      "source": [
        "#### Middle Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utS5sXR5uhVM"
      },
      "source": [
        "#### Last Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2fmSZDuyukgC"
      },
      "outputs": [],
      "source": [
        "ayushVision_MobileNetV3_Model.add(Dense(no_of_classes,activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEiEjznRvZem"
      },
      "source": [
        "## Model Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA3tibbGvc50"
      },
      "source": [
        "#### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1sdUtwB2vf5b"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaWBdoaQv6i8"
      },
      "source": [
        "#### Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3E3gVJfv9UR"
      },
      "source": [
        "##### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pn4gUOczwANx"
      },
      "outputs": [],
      "source": [
        "metrics=[\"accuracy\"]\n",
        "lossfn='categorical_crossentropy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qGgqeXZQvuTS"
      },
      "outputs": [],
      "source": [
        "ayushVision_MobileNetV3_Model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=lossfn,\n",
        "    metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UCcEs7iwfij"
      },
      "source": [
        "### Trainning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FfNVPY1Vwpw8"
      },
      "outputs": [],
      "source": [
        "epochs=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Q19Al6Y-wipO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395cfbe9-f1b9-4aff-ff82-d5e65b2cf1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 13s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "model_history=ayushVision_MobileNetV3_Model.fit(\n",
        "    train_ds_gen,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_ds_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVogOJoXxbEk"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9sCSzfRUxdor",
        "outputId": "0fcaaafd-3b47-4fe2-8850-98fd6a8a37c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = ayushVision_MobileNetV3_Model.evaluate(test_ds_gen)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy"
      ],
      "metadata": {
        "id": "2nwydQm8lP28",
        "outputId": "b7a3e431-043a-4a98-f16e-25bd3f245e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}